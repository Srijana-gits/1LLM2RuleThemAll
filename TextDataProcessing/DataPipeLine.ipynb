{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "#Check torch and tiktoken version\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting pdf into txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/srijanashrestha/Desktop/Projects/Main projects/LLM Project/1LLM2RuleThemAll\n",
      "PDF text extracted and saved to BookAndDataFiles/book.txt\n",
      "Length of text: 3055720\n",
      "First 500 characters:\n",
      " The Lord  of the Rings  \n",
      "BY \n",
      "J.R.R.  Tolkien  \n",
      "\n",
      "Three Rings for the Elven-kings under the sky, \n",
      "Seven for the Dwarf-lords in their halls of stone, \n",
      "Nine for Mortal Men doomed to die, \n",
      "One for the Dark Lord on his dark throne \n",
      "In the Land of Mordor where the Shadows lie. \n",
      "One Ring to rule them all, One Ring to ﬁnd them, \n",
      "One Ring to bring them all and in the darkness bind them \n",
      "In the Land of Mordor where the Shadows lie. \n",
      "CONTENTS  \n",
      "J.R.R.  TOLKIEN i \n",
      "NOTE  ON THE  TEXT  \n",
      "NOTE  ON THE  5 0TH ANN\n"
     ]
    }
   ],
   "source": [
    "#Convert pdf into txt file and save as book.txt\n",
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "# Set working directory to project root\n",
    "os.chdir(\"/Users/srijanashrestha/Desktop/Projects/Main projects/LLM Project/1LLM2RuleThemAll\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(\"BookAndDataFiles\", exist_ok=True)\n",
    "\n",
    "#  Paths\n",
    "pdf_path = \"/Users/srijanashrestha/Desktop/Projects/Main projects/LLM Project/Lord Of The Rings.pdf\"\n",
    "txt_path = \"BookAndDataFiles/book.txt\"\n",
    "\n",
    "# 4Extract text from PDF\n",
    "text = \"\"\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    reader = PyPDF2.PdfReader(f)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:  # skip empty pages\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "\n",
    "# Save text to file\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(f\"PDF text extracted and saved to {txt_path}\")\n",
    "print(\"Length of text:\", len(text))\n",
    "print(\"First 500 characters:\\n\", text[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Lord', 'of', 'the', 'Rings', 'BY', 'J', '.', 'R', '.', 'R', '.', 'Tolkien', 'Three', 'Rings', 'for', 'the', 'Elven-kings', 'under', 'the', 'sky', ',', 'Seven', 'for', 'the', 'Dwarf-lords', 'in', 'their', 'halls', 'of']\n",
      "length=671697\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "\n",
    "print(f\"length={len(preprocessed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size= 23924\n"
     ]
    }
   ],
   "source": [
    "#Converting tokens into IDs\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(f\"vocab size= {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('&', 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "('*', 4)\n",
      "('+', 5)\n",
      "(',', 6)\n",
      "('-', 7)\n",
      "('-B', 8)\n",
      "('-D', 9)\n",
      "('-DO', 10)\n",
      "('-R', 11)\n",
      "('-chebin', 12)\n",
      "('.', 13)\n",
      "('//www', 14)\n",
      "('0TH', 15)\n",
      "('1', 16)\n",
      "('10', 17)\n",
      "('100', 18)\n",
      "('1000', 19)\n",
      "('1001', 20)\n",
      "('1001–11', 21)\n",
      "('1001–4', 22)\n",
      "('1002', 23)\n",
      "('10022', 24)\n",
      "('1003', 25)\n",
      "('1004', 26)\n",
      "('1004–18', 27)\n",
      "('1005', 28)\n",
      "('1006', 29)\n",
      "('1006–16', 30)\n",
      "('1007', 31)\n",
      "('1008', 32)\n",
      "('1009', 33)\n",
      "('1009–17', 34)\n",
      "('100–10', 35)\n",
      "('100–2', 36)\n",
      "('100–6', 37)\n",
      "('101', 38)\n",
      "('1010', 39)\n",
      "('1011', 40)\n",
      "('1012', 41)\n",
      "('1013', 42)\n",
      "('1014', 43)\n",
      "('1014–16', 44)\n",
      "('1015', 45)\n",
      "('1015–16', 46)\n",
      "('1016', 47)\n",
      "('1016–17', 48)\n",
      "('1016–21', 49)\n",
      "('1017', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size=23926\n",
      "('ﬂuttering', 23921)\n",
      "('ﬂy', 23922)\n",
      "('ﬂying', 23923)\n",
      "('<|endoftext|>', 23924)\n",
      "('<|unk|>', 23925)\n"
     ]
    }
   ],
   "source": [
    "##Adding special context tokens\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(f\"vocab size={len(vocab)}\")\n",
    "\n",
    "#printing last five vocabs \n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted tokenizer\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding(BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936434\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/srijanashrestha/Desktop/Projects/Main projects/LLM Project/1LLM2RuleThemAll/BookAndDataFiles/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord  of the Rings  \n",
      "BY \n",
      "J.R.R.  Tolkien  \n",
      "\n",
      "Three Rings for the Elven-kings under the sky, \n",
      "Seven for the Dwarf-lords in their halls of stone,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "enc_sample = enc_text[:50] #removing firsts 50 tokens \n",
    "print(tokenizer.decode(enc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [464, 4453, 220, 286]\n",
      "y: [4453, 220, 286, 262]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464] ----> 4453\n",
      "[464, 4453] ----> 220\n",
      "[464, 4453, 220] ----> 286\n",
      "[464, 4453, 220, 286] ----> 262\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ---->  Lord\n",
      "The Lord ---->  \n",
      "The Lord  ---->  of\n",
      "The Lord  of ---->  the\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dataset for batched inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                        stride=128, shuffle=True, drop_last=True,\n",
    "                        num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 464, 4453,  220,  286]]), tensor([[4453,  220,  286,  262]])]\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"BookAndDataFiles/book.txt\"\n",
    "\n",
    "with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[4453,  220,  286,  262]]), tensor([[  220,   286,   262, 26028]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  464,  4453,   220,   286],\n",
      "        [  262, 26028,   220,   220],\n",
      "        [  198, 17513,   220,   198],\n",
      "        [   41,    13,    49,    13],\n",
      "        [   49,    13,   220, 32447],\n",
      "        [  220,   220,   198,   198],\n",
      "        [12510, 26028,   329,   262],\n",
      "        [40748,    12,    74,   654]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 4453,   220,   286,   262],\n",
      "        [26028,   220,   220,   198],\n",
      "        [17513,   220,   198,    41],\n",
      "        [   13,    49,    13,    49],\n",
      "        [   13,   220, 32447,   220],\n",
      "        [  220,   198,   198, 12510],\n",
      "        [26028,   329,   262, 40748],\n",
      "        [   12,    74,   654,   739]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  464,  4453,   220,   286],\n",
      "        [  262, 26028,   220,   220],\n",
      "        [  198, 17513,   220,   198],\n",
      "        [   41,    13,    49,    13],\n",
      "        [   49,    13,   220, 32447],\n",
      "        [  220,   220,   198,   198],\n",
      "        [12510, 26028,   329,   262],\n",
      "        [40748,    12,    74,   654]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=max_length,\n",
    "stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "with open(\"/Users/srijanashrestha/Desktop/Projects/Main projects/LLM Project/1LLM2RuleThemAll/BookAndDataFiles/book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(text)\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                        stride=128, shuffle=True, drop_last=True,\n",
    "                        num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
